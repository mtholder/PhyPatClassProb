\documentclass[11pt]{article}
\usepackage{graphicx}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{wrapfig}
\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in
\usepackage{setspace}
\singlespacing

\usepackage{titlesec}
\usepackage{paralist} %compactenum

%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}{Definition}
\usepackage{tipa}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage[mathscr]{eucal}

\titlespacing*{\section}{0cm}{1mm}{-3mm}
\titlespacing*{\subsection}{0cm}{-3mm}{-3mm}
% Use the natbib package for the bibliography
\usepackage[round]{natbib}
\bibliographystyle{apalike} 
\newcommand{\exampleMacro}[1]{\mu_{#1}}



\usepackage{xspace}
%\usepackage{ulem}
\usepackage{url}
%\usepackage{ensuremath}
\usepackage{hyperref}
\usepackage{amsfonts}





\newcommand{\treeRoot}{\ensuremath{\rho}\xspace}
\newcommand{\numObserved}{\ensuremath{s}\xspace}
\newcommand{\inform}{\ensuremath{I}\xspace}
\newcommand{\uninform}{\ensuremath{U}\xspace}
\newcommand{\numStates}{\ensuremath{K}\xspace}
\newcommand{\numLeavesTotal}{\ensuremath{N}\xspace}
\newcommand{\allStates}{\ensuremath{\mathcal S}\xspace}
\newcommand{\allStateSets}{\ensuremath{\mathcal G}\xspace}
\newcommand{\allStateSetsTruncated}{\ensuremath{\mathcal H}\xspace}
\newcommand{\comboSet}{\ensuremath{\mathcal X}\xspace}
\newcommand{\edgeLengths}{\ensuremath{\bm \nu}\xspace}
\newcommand{\patProbSym}{\ensuremath{\mathbb P}\xspace}
\renewcommand{\Pr}{\patProbSym}
\newcommand{\patProb}[6]{\ensuremath{\patProbSym\left(#1,#2,#3,#4,#5,#6\right)}\xspace}
\newcommand{\patProbFull}[7]{\ensuremath{\patProbSym\left(#1,#2,#3,#4,#5,#6,#7\right)}\xspace}
\newcommand{\patClassSym}{\ensuremath{\mathcal C}\xspace}
\newcommand{\patClass}[3]{\ensuremath{\patClassSym_{#1,#2,#3}}\xspace}
\newcommand{\probUninformPatClassSym}{\ensuremath{\mathbb U}\xspace}
\newcommand{\probUninformPatClass}[5]{\ensuremath{\probUninformPatClassSym_{#1,#2,#3,#4}\left(#5\right)}\xspace}
\newcommand{\informPatClassSym}{\ensuremath{\mathcal U}\xspace}
\newcommand{\informPatClass}[5]{\ensuremath{\informPatClassSym_{#1,#2,#3,#4,#5}}\xspace}
\newcommand{\stateOf}[1]{\ensuremath{c\left(#1\right)}\xspace}
\newcommand{\leftChild}[1]{\ensuremath{a\left[#1\right]}\xspace} % a and b used below without macro
\newcommand{\rightChild}[1]{\ensuremath{b\left[#1\right]}\xspace}% a and b used below without macro
\newcommand{\shared}[1]{\ensuremath{r\left(#1\right)}\xspace}
\newcommand{\numLeaves}[1]{\ensuremath{t\left(#1\right)}\xspace}
\newcommand{\partitionSet}[0]{\ensuremath{\mathcal P}\xspace}
\newcommand{\subsetsOfSizeSet}[3]{\ensuremath{\mathcal R}\left(#1,#2,#3\right)\xspace}

\newcommand{\order}{{\mathcal{O}}}
\newcommand{\expectation}{{\mathbb{E}}}
\newcommand{\expect}[1]{\expectation\left[#1\right]}
\newcommand{\pvalue}{$P$-value\xspace}
\newcommand{\pvalues}{$P$-values\xspace}
\newcommand{\myFigLabel}[1]{\stepcounter{myFigCounter} \newcounter{#1} \setcounter{#1}{\value{myFigCounter}} Fig \arabic{#1}}
\newcommand{\mthNote}[1]{{\color{red}#1}\\}
\newcommand{\FelsensteinPruneSym}{{\mathcal{F}}}
\newcommand{\FelsensteinPruneUninform}[5]{\FelsensteinPruneSym(#1,#2,\left[#3,#4,#5\right])}
\newcommand{\DoubleFelsensteinPruneSym}{{\mathcal{E}}}
\newcommand{\DoubleFelsensteinPruneUninform}[8]{\DoubleFelsensteinPruneSym(#1,#2,\left[#3,#4,#5\right],\left[#6,#7,#8\right])}
\hypersetup{backref,  pdfpagemode=FullScreen,  linkcolor=blue, citecolor=red, colorlinks=true, hyperindex=true}

\newcounter{myFigCounter}
\begin{document}
\tableofcontents
\section{Calculating the probability of classes of data pattern}
Given:
\begin{compactitem}
	\item a bifurcating (``binary'') tree, $T$, with root node (vertex) denoted \treeRoot,
	\item with a set of edge lengths, \edgeLengths, defined (not to be estimated), and
	\item  a fully parameterized model of character state change (parameters of the model $\bm\theta$ are fixed, and do not need to be estimated),
\end{compactitem}
we would like to be able to calculate the probability of events such as: $\Pr(x\in\patClass{i}{j}{k}(n) \mid T, \edgeLengths, \theta)$ where $x$ refers to a character (a column in a taxa by characters data matrix) and \patClassSym refers to a class of possible data patterns.
The indexing and notation for referring to the class of patterns is as follows:
\begin{compactitem}
	\item argument in parentheses, $n$: a node. All subscripts described below refer to properties of the subtree rooted at this node;
	\item first subscript, $i$: \inform for parsimony-informative or \uninform for parsimony-uninformative;
	\item second subscript, $j$: the set of states observed in tips that are descendants of this node;
	\item third subscript, $k$: the parsimony length of the subtree rooted at the node.
\end{compactitem}
For the sake of brevity this probability will be written as $\Pr(x\in\patClass{i}{j}{k}(\treeRoot))$ with the dependence on  $T, \edgeLengths,$ and $\theta$ implied.

We will demonstrate that we can calculate this probability in an algorithm that scales XXX with respect to $\numLeavesTotal$, the number of leaves in the tree and exponentially with respect to $\numStates$ the number of character states possible in each column.


For parsimony-uninformative patterns, $\Pr(x\in\patClass{\uninform}{j}{|j|-1}(\treeRoot))$ can be calculated as:
\begin{eqnarray}
   \sum_{m\in\allStates}\probUninformPatClass{j}{\emptyset}{j}{m}{\treeRoot}\Pr(m|\theta) && 	\forall j \in  \subsetsOfSizeSet{\allStates}{\numLeavesTotal}{\numStates} \mbox{ if } N = |j|  \label{noRepeatedStates}\\
  \sum_{r\in j}\sum_{f\in\subsetsOfSizeSet{\allStates-r}{0}{\numLeavesTotal-1}}\sum_{m\in\allStates}\probUninformPatClass{j}{\{r\}}{r\cup f}{m}{\treeRoot}\Pr(m|\theta)  &&  \mbox{ if } N > |j| \mbox{ and }\forall j \in  \subsetsOfSizeSet{\allStates}{1}{\numLeavesTotal-1}\label{oneRepeatedState}\\
  0 && \mbox{ for all other } j \mbox{}
\end{eqnarray}

where
\begin{compactitem}
	\item $\probUninformPatClass{j}{r}{f}{m}{n}$ is the probability that the character will be a parsimony-uninformative pattern for the subtree rooted an $n$ with:
		\begin{compactitem}
			\item the set $j$ of states observed at the tips of the subtree,
			\item the set $r$ of states observed in more than one tip in the subtree,
			\item the Fitch down-pass state set of $f$ at node $n$, and
			\item ancestral state of $m$ at node $n$.
		\end{compactitem}
		Although \probUninformPatClassSym is a probability conditioned on $T, \edgeLengths,$ and $\theta$ this dependence is dropped from the notation for the sake of brevity.
	\item $\Pr(m|\theta)$ is the prior probability of state $m$ given the model parameters.
	\item \allStates denotes the set of all states.
	\item $\subsetsOfSizeSet{\mathcal{Z}}{x}{y}$ denotes the set of all subsets of ${\mathcal Z}$ which have sizes in $[x,y]$
\end{compactitem}

Formula (\ref{noRepeatedStates}) covers parsimony-uninformative cases in which no state is repeated on the tree (this only occurs when $\numLeavesTotal\leq\numStates$).
Formula (\ref{oneRepeatedState}) covers parsimony-uninformative cases in which one state is repeated on the tree.
It is impossible to have a parsimony-uninformative pattern in with more than one state that is shared by multiple tips.
Note that in all cases, the non-zero probabilities are only obtained when the parsimony-length of the tree is equal to the minimum possible length ($|j| - 1$), as is required by a parsimony-uninformative character.

\subsection{Recursive calculations for the probability of patterns}
\subsubsection{Leaves}
If node $n$ is a leaf, then there is no opportunity for the state at $n$ to change in the subtree rooted at $n$.
In other words, the state at node $n$ will be only member of the set of states seen within the subtree; this state will be the only member of the Fitch down-pass state set.
No state will be repeated, so the set of states observed more than once in the subtree must be the empty set.
From these properties, we can conclude that for any leaf $n$, and for all $m \in \allStates$:
\begin{eqnarray}
	\probUninformPatClass{\{m\}}{\emptyset}{\{m\}}{m}{n} = 1 
\end{eqnarray}
and $\probUninformPatClass{j}{r}{f}{m}{n} = 0$ if $j\neq\{m\}$, $r\neq\emptyset$, or $f\neq\{m\}$.


\subsubsection{Uninformative patterns at internal nodes}
A pattern that is informative for a subset of the taxa will automatically be informative for the entire set, so the elements of the calculation of $\probUninformPatClassSym$ for an internal node can be expressed from the probabilities of uninformative patterns of the children of the node.

Let \leftChild{n} and \rightChild{n} denote the left and right children of node $n$, respectively.
To maintain the convention that that $j$ refers to the set of observed states in a subtree, we will use $a_j$ and $b_j$ to refer to the subset of states of observed in the left child's subtree and the right child's subtree, respectively.
A similar convention will be used for the subcripts $r,f,$ and $m$.
Note that 
\begin{eqnarray}
	j & = &  a_j \cup b_j \\
	r & = & a_r \cup b_r\cup(a_j \cap b_j)\\
	f & = & \left\{\begin{array}{cl}
				a_f \cup b_f & \mbox{if } a_f \cup b_f =\emptyset \\
				a_f \cap b_f & \mbox{otherwise}
		  \end{array}\right.
\end{eqnarray}
Recall the a pattern is parsimony-informative it more than 1 state is shared by more than one leaf on the tree.
Thus $|r| < 2$ for all parsimony-uninformative patterns.



\subsubsection{Unoptimized, worst-case Uninformative data pattern calculations}
\begin{table}[htdp]
\caption{Worst-case size of each dimension in our probability lookup table for any node for uninformative patterns}
\begin{center}
\begin{tabular}{|c|c|p{4in}|}
\hline
index & dimension & explanation\\
\hline
informative index, $i$ & 1 & $i=\uninform$ \\
obs.\ state set, $j$ & $2^K-1$ & any state set except $\emptyset$\\
repeated state set, $r$ & K + 1 & $\emptyset$, or a maximum of one state repeated (because $i=\uninform$)\\
Fitch down pass, $f$ & $2^{|j|}-1$ & must be a subset of the states observed  \\
ancestral state, $m$ & K & any possible state could be present at the ancestor \\
\hline
\end{tabular}
\end{center}
\label{default}
\end{table}%

This implies memory usage of $\left(2^K-1\right)\left(K^2 + K\right)\left(2^{|j|}-1\right)$ for the uninformative patterns for each internal node.
Because $|j|$ is on the order of $K$, and the number of internal nodes is $N - 1$, the total memory usage for uninformative patterns is on the order of $\order\left(NK^22^{2K}\right)$

Fortunately, some of the probability bins will always correspond to probabilities of 0, and can thus be skipped. 
This will save memory, and could save calculation time.
Namely the size of the observed state set can never exceed the number of leaves below node $n$.  
So the number of elements in index $j$ will be limited by $2^{\min(K,\numLeaves{n})}-1$ rather than $2^K-1$, where $\numLeaves{n}$ is the number of leaves in the subtree rooted at $n$.
Similarly, $r$ must be indexed only up to $\min(K,\numLeaves{n}) + 1$.
For balanced trees, a large number of nodes are shallow (close to the leaves of the tree), so this savings can be substantial.

Furthermore, note that if $r=\emptyset$, then $f=j$ for uninformative patterns; If there has been no shared state, then every traversed node must have resulted in a union in the Fitch algorithm.
And if $r\neq\emptyset$, then we know that the repeated state found in node $n$ must always be in the node's Fitch downpass set.
Thus, the indices of $f$ for the of possible bins of probability with non-zero terms is limited by $2^{|j|-1}-1$ rather than $2^{|j|}-1$.

Thus, for an internal node $n$,  we can calculate \probUninformPatClass{j}{r}{f}{m}{n} by setting all of the bins to zero and then replacing 0 with the results of the calculations shown below; in other words, all combinations of indices not mentioned below will have probability of 0.0.

We will use the $\FelsensteinPruneSym$ to denote use of a pruning calculation for a single child lineage for uninformative patterns:
\begin{eqnarray}
	\FelsensteinPruneUninform{n}{m}{a_j}{a_r}{a_f} = \Pr(a_m|\edgeLengths, m)\probUninformPatClass{a_j}{a_r}{a_f}{a_m}{\leftChild{n}}
\end{eqnarray}
We will use the $\DoubleFelsensteinPruneSym$ to denote use of a pruning calculation for a product for a pairs of sister lineages for uninformative patterns:
\begin{eqnarray}
	\DoubleFelsensteinPruneUninform{n}{m}{a_j}{a_r}{a_f}{b_j}{b_r}{b_f} = \Pr(a_m|\edgeLengths, m)\probUninformPatClass{a_j}{a_r}{a_f}{a_m}{\leftChild{n}}\Pr(b_m|\edgeLengths, m)\probUninformPatClass{b_j}{b_r}{b_f}{b_m}{\rightChild{n}}
\end{eqnarray}
This operation contributes the probability associated with state $m$ in the ancestor evolving to be state $a_m$ in the left child, and $b_m$ in the right child, with the appropriate indexing of the children's probability statements for the other indices.

\subsubsection{Parsimony-uninformative, no repeated state}
If $\numLeaves{n}\leq\numStates$, then for all $j\in\subsetsOfSizeSet{\allStates}{\numLeaves{n}}{\numLeaves{n}}$
\begin{eqnarray}
\probUninformPatClass{j}{\emptyset}{j}{m}{n}= \sum_{a_j\in\subsetsOfSizeSet{j}{\numLeaves{\leftChild{n}}}{\numLeaves{\leftChild{n}}}}\DoubleFelsensteinPruneUninform{n}{m}{a_j}{\emptyset}{a_j}{j-a_j}{\emptyset}{j-a_j}
\end{eqnarray}
Note that there will be $\numStates\choose\numLeaves{n}$ distinct sets that could constitute valid values for $j$, and for each $j$ the summation will include $\numLeaves{n}\choose\numLeaves{\leftChild{n}}$ terms.



\subsubsection{Parsimony-uninformative, repeated state}
For all internal nodes, there is some probability of a parsimony-uninformative pattern in which a state is observed in multiple leaves.
If $s$ represents the repeated state ($r=\{s\}$) then $s\in \allStates$, there are potentially many sets of ``other'' (non $s$) states observed: $o\in\subsetsOfSizeSet{\allStates-s}{\numLeaves{n}-2}{\numLeaves{n}-2}$.


\subsubsection{Parsimony-uninformative, repeated state, downpass set of size 1}
If and only if, $s$ is present in both the left and right subtrees of $n$, then the downpass state set of $n$ will consist of $\{s\}$ only.
The non-repeated states present in $o$ cannot occur in both the left and right subtrees, so we can use $a_o$, and $b_o$ to denote the states contributed by each subtree. We note that $a_o$ and $b_o$ form a bipartition of $o$.
The subtree $\rightChild{n}$ can contribute at most $\numLeaves{\rightChild{n}}-1$ states to $o$ (in other words $|b_o| \leq \numLeaves{\rightChild{n}}-1$) because state $s$ must be present in at least one leaf.
Thus, we must consider all $a_o\in\subsetsOfSizeSet{o}{|o| +1 - \numLeaves{\rightChild{n}} }{\numLeaves{\leftChild{n}}-1}$.
Once we know $o$ and $a_o$, then we know that $b_o = o - a_o$.
Finally we must consider all possible downpass state sets for $\leftChild{n}$ and $\rightChild{n}$.
$s$ must be a member of the downpass states set (because it is present, and no other state is repeated).
If $a_{p}$ denotes the set of states other than $s$ that are in the downpass state set of $\leftChild{n}$, then we must consider every 
$a_p\in\subsetsOfSizeSet{a_o}{0}{\numLeaves{\leftChild{n}}-1}$ and every $b_p\in\subsetsOfSizeSet{b_o}{0}{\numLeaves{\rightChild{n}}-1}$

Thus, $\forall s\in \allStates, \forall o\in\subsetsOfSizeSet{\allStates-s}{\numLeaves{n}-2}{\numLeaves{n}-2}$:
\begin{eqnarray}
\mbox{let }r & =&  \{s\}   \nonumber \\
\probUninformPatClass{r\cup o}{r}{r}{m}{n} &= & \sum_{a_o}\left[\sum_{a_p}\FelsensteinPruneUninform{\leftChild{n}}{m}{a_o\cup r}{A_R}{a_o\cup r } \right]\left[\sum_{b_p}\FelsensteinPruneUninform{\rightChild{n}}{m}{b_o\cup r}{B_R}{b_o\cup r } \right]\nonumber
\end{eqnarray}
Where $A_R$ is either the empty set if $|a_o| + 1 = \numLeaves{\leftChild{n}}$ or $\{s\}$ if $|a_o| + 1 < \numLeaves{\leftChild{n}}$. $B_R$ is defined similarly for the right child.


For every $s\in \allStates$, we must consider the constant-$s$ pattern in the subtree, for every $m\in \allStates$ possible at the ancestral node $n$:
\begin{eqnarray*}
	\probUninformPatClass{\{s\}}{\{s\}}{\{s\}}{m}{n}= \left(\sum_{a_m\in\allStates}\Pr(a_m|\edgeLengths, m)\probUninformPatClass{\{s\}}{J}{\{s\}}{a_m}{\leftChild{n}}\right)\left(\sum_{b_m\in\allStates}\Pr(b_m|\edgeLengths, m)\probUninformPatClass{\{s\}}{J}{\{s\}}{b_m}{\rightChild{n}}\right)
\end{eqnarray*}
where $J$ denotes the shared state set $j$ if there are multiple children in the subtree or $\emptyset$ if the child subtree is a leaf.



For any node for which $\numLeaves{n} \leq K$, we must consider cases in which $r = \emptyset$. All of these will have parsimony length of $\numLeaves{n} - 1$. 
Furthermore this will only happen when the observed state set (and the fitch downpass state set) are subsets of $\allStates$ that have size $\numLeaves{n}$.
Let $\subsetsOfSizeSet{\allStates}{\numLeaves{n}}{\numStates}$ denote all of the subsets of $\allStates$ that have a size $\numLeaves{n}$.
Note that there are $K\choose\numLeaves{n}$ such subsets.
So for every $m\in \allStates$ and every $j \in \subsetsOfSizeSet{\allStates}{\numLeaves{n}}{\numStates}$ we must calculate  as:
\begin{eqnarray*}
\patProbFull{\uninform}{j}{\emptyset}{\numLeaves{n} - 1}{j}{m}{n} = \sum_{[{a_j}, { b_j}]\in\partitionSet(j,\numLeaves{\leftChild{n}},\numLeaves{\rightChild{n}})}  & & \left[\sum_{a_m\in\allStates} \Pr(a_m|\edgeLengths, m)\patProbFull{\uninform}{a_j}{\emptyset}{0}{a_j}{a_m}{\leftChild{n}}\right] \\
& \ldots& \left[\sum_{b_m\in \allStates}\Pr(b_m|\edgeLengths, m)\patProbFull{\uninform}{b_j}{\emptyset}{0}{b_j}{b_m}{\rightChild{n}}\right]
\end{eqnarray*}
where $\partitionSet(j ,y,z)$ denotes the set of all pairs of sets $a,b$ such that $a \cup b = j$, $a\cap b = \emptyset$, $|a| \geq y$, and $|b| \geq z$.







For the typical case (in which $\numLeavesTotal >\numStates$)


Note that the subscripting of \patClassSym ensures that the classes of interest are mutually exclusive.
Thus if we traverse the tree in post-order (visiting nodes from tips to root), we can calculate probabilities for each possible class of patterns by summing the probabilities of patterns.
As with Felsenstein's pruning algorithm \citep{Felsenstein1981a}, we must consider each possible state at each ancestral node.
In order to combine partial patterns for child nodes into the appropriate class at their parent's node, we must also maintain the set of states in the subtree.

For each node, $n$, we will calculate and store \patProb{i}{j}{r}{k}{m}{n} where:
\begin{compactitem}
	\item $i$ is \inform  or \uninform,
	\item $j$ is a set of states observed in the tips of the subtree rooted at node $n$,
	\item $r$ is the set of states that are observed more than one time in the tips of the subtree rooted at node $n$,
	\item $k$ is the parsimony length of the subtree rooted at node $n$,
	\item $m$ is a an ancestral character state (a latent variable in this system, because we do not observe these states).
\end{compactitem}
\patProb{i}{j}{r}{k}{m}{n} is defined as
\begin{equation}
	\sum_{r\in\allStateSets}\patProb{i}{j}{r}{k}{m}{n} = \Pr\left(x\in\patClass{i}{|j|}{k}(n) \mid T, \edgeLengths, \theta, \stateOf{n}=m\right)
\end{equation}
\mthNote{This should be phrased by indexing $\patClassSym$ by $r$ as well to make a stronger statement.}
Where \stateOf{x} denotes the character state of node $n$



\subsection{Internal nodes}
Then we add probabilities corresponding to all the distinct ways that the class of patterns can be achieved.
Informally, we can think of  as
\begin{eqnarray*}
	&\patProb{i}{j}{r}{k}{m}{n} =  \sum_{[{\bm a}, {\bm b}]\in\comboSet(i, j, r, k)} & \left[\patProb{a_i}{a_j}{a_r}{a_k}{a_m}{a(n)}\Pr(\stateOf{\leftChild{n}}=a_m \mid \edgeLengths,\stateOf{n}=m)\ldots\right. \\
	 & & \,\left.\patProb{b_i}{b_j}{a_r}{b_k}{b_m}{b(n)}\Pr(\stateOf{\rightChild{n}}=b_m \mid \edgeLengths, \stateOf{n}=m)\right] \forall m \in\allStates
\end{eqnarray*}
where $\comboSet(i, j, r, k)$ is the set of all pairs of child index arrays that could result in the index $i,j,r,k,m$ at their parent node, $n$.
The indices for the children are shown as subscripts; for example, we use $a_i$ for the $i$ index when we are referring to child \leftChild{n}.

Note that the probabilities such as $\Pr(\stateOf{\leftChild{n}}=a_m \mid \edgeLengths,\stateOf{n}=m)$ refer to a transition from the state $m\rightarrow a_m$ across a single edge in the tree. For the sake of brevity this will be denoted $\Pr(a_m|\edgeLengths, m)$
These probabilities are the same transition probabilities that are typically used in phylogenetics, so these quantities can easily be calculated (in constant time if their is an analytical solution for the model of character evolution and in $\order(K^3)$ time where $K$ is the number of states, $K=|\allStates|$).

We can calculate the probabilities for ancestral nodes by first assigning a probability of 0 to every bin (each \patProb{i}{j}{r}{k}{m}{n} for the internal node $n$).
Then we must add in the contribution for every combination within $\comboSet(i, j, r, k)$.

Note that the set of states observed in the subtree rooted at node $n$ is simply the union of the corresponding state set in the children, so
$$	a_j \cup b_j = j $$
for all $[{\bm a}, {\bm b}]\in\comboSet(i, j, k)$


\mthNote{At this point, I remembered that all of the probability statements will have to be indexed by the Fitch downpass state set, as well.  I have not added that to the notation above, but will use $f$ for the Fitch downpass state set of node $n$ (and $a_f$ and $b_f$ for the children's state sets).}
Also note that (because of the fact that the Fitch algorithm can add at most one state to the tree length for each node), $k$, the tree length below node $n$ will either be the sum of the subtrees, or will be this sum with another count.
So
 $$k = a_k + b_k\mbox{ and } d = a_d \cap b_d$$
 if $a_d \cap b_d \neq \emptyset$, and
  $$k = a_k + b_k + 1\mbox{ and } d = a_d \cup b_d$$
 if $a_d \cap b_d = \emptyset$.

While it may seem that the state at the internal node ($m$ above) would affect which elements of $\comboSet(i, j, r, k)$ are valid combinations, this is not the case if we only consider trees in which all of the edges have non-zero length (as is typically done).



We use the following rules for which probability ``bins'' in the children contribute to an ancestor's pattern class probabilities:













\section{{\color{red}Future work - notes below here are not in a finished state}}
\subsection{Patterns that support a branch.}
Here we consider the following problem: what is the probability of generating a pattern that is shorter on $T_1$ than on $T_{e2}$ or $T_{e3}$ where $T_{e2}$ and $T_{e3}$ are the two distinct tree topologies that are NNI neighbors of tree $T_1$ but do not contain edge $e$.
Such a character would ``support'' edge $e$.
The set of such patterns will be denoted $\mathcal{S}$.

Note that we are not guaranteeting that all (or even that {\em any}) minimal-length trees  for the character contain edge $e$. 
Rather we are assessing a form of local support in which we consider the rest of the tree to be provisionally correct, and are interested in how many characters vary in length across the edge.


Let $\ell_i$ be the length of the character on tree $i$ across the 5 edges of interest (all edges adjacent to the nodes connected by $e$).

Without loss of generality, we will root the tree, $T_1$ across the edge of interest.
The subtrees of $T_1$ will be $((a,b),(c,d))$. The subtrees of $T_2$ will be $((a,c),(b,d))$; The subtrees of $T_3$ will be arranged $((a,d),(b,c))$




Note that $\ell_i \in \{0, 1, 2, 3\}$ because there are three Fitch down-pass nodes considered when sweeping over this part of the tree (thus there can be a maximum of 3 steps added because of the configuration of $a,b,c,d$.
We will prove that, for all patterns in $\mathcal{S}$
\begin{compactenum}
	\item  $\ell_1 \neq 0$  (intersections present between all four subtrees $\rightarrow \ell_i = 1 \forall i$
	\item  $\ell_1 \neq 2$
	\item  $\ell_1 \neq 3$ ( $\ell_2 \leq 3$ and $\ell_3 \leq 3$, so if $\ell_1=3$ then the character cannot be shorter on $T_1$
\end{compactenum}

\bibliography{phylo}
\end{document}







Discarded text




So $\probUninformPatClass{a_j}{a_r}{a_f}{a_m}{\leftChild{n}}$ and $\probUninformPatClass{b_j}{b_r}{b_f}{b_m}{\rightChild{n}}$ will contribute to
$\probUninformPatClass{j}{r}{f}{m}{n}$ if and only if:
\begin{eqnarray}
	|a_j \cap b_j | < 2 \label{maxOneSharedAcrossSubtrees}\\
	|a_r \cup b_r | < 2. \label{notDifferentStatesSharedAcrossSubtrees}
\end{eqnarray}
If inequality (\ref{maxOneSharedAcrossSubtrees}) were not obeyed then there would be states that are seen in each of the subtrees (resulting in a parsimony informative pattern).
If inequality (\ref{notDifferentStatesSharedAcrossSubtrees}) were not obeyed then each subtree ``contributes'' a distinct state that is shared across multiple leaves, so that character for the entire subtree rooted an $n$ would be a parsimony informative pattern.



